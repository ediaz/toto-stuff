\newpage
\section{Proposed research}

In this section, I review in more detail the concepts developed in each 
of the projects of the proposal. Most projects share a similar background
technology like modeling operators (two-way or Marchenko), extended, and 
conventional imaging. On the tomographic side, all the methods
share the constraints on the wave equation. This constraints implies that all
computed wavefields follow the wave equation. Hence,  the 
obtained objective function gradients share the steps of the adjoint step method 
described in \cite{plessix}. 

\section{1- CIP wavefield tomography with illumination compensation}

Extended-image gathers~\citep{rickett:883,SavaVasconselos}
highlight the spatial and temporal consistency between wavefields by exploring
 the focusing information in the image domain. 
 The moveout in the gather is sensitive to
velocity perturbations, and hence can be optimized. The most
general extended gather can be defined as follows:
\beq
R(\xx,\hh,\tau)= \sum_{{e}} \sum_{t} \US({e},\xx - \hh,t-\tau) \UR({e},\xx+\hh,t+\tau),
\label{eq:eic}
\eeq
 where $\hh$ is the space-lag vector, $\tau$ the time-lag, $\xx$ the image location, $e$ the
experiment index, $\US$ the 
 source wavefield, and $\UR$ the receiver wavefield. Given the increase of dimensions 
for extensions in space and time, one can take advantage of a sparse sampling 
of the extended images in a subset of image locations $\xx_c$ instead of the full image space $\xx$. One smart
way to  decide the locations $\xx_c$ is by placing the observation points at the reflector
locations \citep{cullison}.  
 Note that the process for computing the extended
image is linear with respect to one of the wavefields. Hence, one can 
define the extended image in a matrix-vector form as 
\beq
  {\bf R} = {\bf I}_{s} {\bf u}_r = {\bf I}_{r} {\bf u}_s,
\label{eq:eicops}
\eeq
where $\bf R$ is the vector representation of the extended image, and 
 ${\bf I}_s$ and ${\bf I}_r$ are the imaging operators for the source and receiver wavefield,
respectively. 

Even though the commutation of the forward mappings (Equation~\ref{eq:eicops})
 produces the same result ($\bf R$), the adjoint 
mapping satisfies different equations. The operator ${\bf I}_s^\top$ implements
\beq
  \tilde{\UR}(e,\xx+\hh,t+\tau) += \sum_{\tau,\hh}R(\xx,\hh,\tau) \US(e,\xx - \hh,t-\tau),
\label{eq:adj1}
\eeq
whereas ${\bf I}_r^\top$ satisfies
\beq
  \tilde{\US}(e,\xx-\hh,t-\tau) += \sum_{\tau,\hh}R(\xx,\hh,\tau) \UR(e,\xx + \hh,t+\tau).
\eeq
The symbol $+=$ implies that the adjoint wavefield is accumulated in the output after
summing all possible lags $\hh,\tau$.


The source and receiver wavefields follow the wave equation, which in this chapter is the
acoustic wave equation: 
\beq
  \swe{u(\xx,t)} = f(\xx,t),
\label{eq:we}
\eeq
where $\m=s^2(\xx)$ is the squared slowness. The source wavefield $\US$ is computed by forward
propagation of the source function $\fs$, whereas the receiver wavefield $\UR$ is created by backward (adjoint)
propagation of the recorded data $\fr$ as summarized by the following system of equations:
\beq \label{eqn:FSV}
\MAT{ \L(m,t) & 0 \\ 0  & \L^\top(m,t)}
\MAT{ \us \\ \ur} =
\MAT{ \fs \\ \fr} ,
\eeq
where $\L$ and $\L^\top$ are the matrix representation of the forward and adjoint (reverse) propagators, respectively. 

One can learn a great deal about the velocity model by looking at the focusing information
encoded in the extended images. A model with a good kinematic agreement with 
the true background velocity exhibits 
a maximum correlation between source and receiver wavefields around $(\hh,\tau)=({\bf 0},0)$. Under velocity 
inaccuracies, the
extended gathers exhibit a moveout that is indicative of 
 errors in the model~\citep{YangSava:moveout}. Hence,
 the energy on the gathers can be brought around zero lag through an
inversion process in which the velocity model can be updated. A typical 
objective function \citep{ShenSymes.geo.2008,Wiktor,tony:gp15} based on 
extended images is 
\beq
 J_1(\m) = \norm{P(\hh,\tau) R(\xx,\hh,\tau)}^2,
\eeq
where $P$ is a penalty operator whose purpose is to 
highlight the events in the image that show velocity inaccuracies. 
 The penalty operator is sometimes defined using the lags:
\beq
  P(\hh,\tau) = \sqrt{ \hh^2 +(v\tau)^2},
\eeq
where $v$ is the local velocity \citep{tony:gp15}. This penalty function is 
highly idealized since it produces a zero residual if the 
extended image is a perfect spike (i.e. $R(\xx,\hh,\tau)=\delta(\xx,\hh,\tau)$).
 In order to relax such requirements, \cite{tony_seg:cwp12} propose
a penalty function based on illumination, which depends on the 
acquisition setup, data bandwidth, and the velocity model itself. 
 To compute the action of the illumination (blurring  
operator) one can cascade a demigration/migration process 
applied to spikes placed at the extended image locations $\xx_c$:
\beq
  \tilde{R}(\xx_c,\hh,\tau) = {\bf M} {\bf M}^\top \sum_{i} \delta(\xx-\xx_c^i,\hh,\tau)
\eeq
with $\bf M^\top$ being a demigration operator and $\bf M$ the extended imaging process. The index
$i$ corresponds to the CIP location $\xx_i$.
 The synthetic image $\tilde{R}$ contains the action of the source function, the acquisition, 
and the model at the image locations $\xx_c$. Once $\tilde{R}$ is obtained, one can 
proceed with the penalty 
\beq
  \tilde{P}(\xx_c,\hh,\tau) = \frac{1}{Env(\tilde{R}(\xx_c,\hh,\tau))},
\eeq
where $Env$ is an envelope function that captures the energy of the synthetic  
image $\tilde{R}$.
Effectively, $\tilde{R}$ contains the extended point spread functions (PSF) of the model, 
and is computed similarly to \cite{valenciano:2009} and \cite{FletcherLSM}.


In order to include the zero-lag information one can add an extra term in the objective 
function, similarly to \cite{shen:VE49}, \cite{Wiktor}. Alternatively, I  
include the zero-lag term directly into the inversion by changing the goal 
of the objective function to a minimization problem~\citep{Zhang}:
\beq
 J_2(\m) = -\norm{H(\hh,\tau) R(\xx,\hh,\tau)}^2.
\label{eq:j2}
\eeq
 The operator $H$ has the opposite purpose from operator $P$; 
now the main task of $H$ is to ``highlight'' the energy around zero lag. By using $J_2$ instead of 
$J_1$, we fuse all lags in a seamless manner into the inversion. Another advantage of
$J_2$ is that avoids the extra image power term in the inversion (to include zero lag). It also
avoids assigning an weighting scalar value to balance the DSO and image power terms.
%%


\subsection{Tomography setup}
We are interested in the gradient of equation \ref{eq:j2} to update the background velocity
model. We setup the problem by including constraints on the wave operators:
 The inverse problem is setup with constraints on the wave equation:
\beq
  \begin{aligned}
    & \underset{\m}{\text{minimize}}
    & & J_2(\m) \\
    & \text{subject to}
    & & \L \US = \fs, 
    & & \L^\top \UR = \fr.
  \end{aligned}
\eeq
In order to compute the gradient 
I follow the methodology described by \cite{plessix} for PDE constrained 
objective functions. First, we define the augmented functional
\beq
  A(\m,\us,\ur,a_s,a_r) = -J_2(\m) + < a_s,\L\us-\fs> +<a_r,\L^\top\ur-\fr>,
\label{eq:aug1}
\eeq
where $<.,.>$ defines a dot product between vectors in  the $(\xx,t)$ space, $a_s$ is the 
adjoint source wavefield, and $a_r$ is the adjoint receiver wavefield. 
The augmented functional depends on the model $\m$, the forward state variables
$\us$ and $\ur$, and the adjoint state variables $a_s$ and $a_r$.
 From the  first optimality conditions, $\frac{\partial A }{\partial a_s}=0$
and $\frac{\partial A }{\partial a_r}=0$, 
 we obtain the system in equations \ref{eqn:FSV}. 

In order to find the adjoint source wavefield $a_s$, I rewrite equation \ref{eq:aug1} as:
\beq
    A(\m,\us,\ur,a_s,a_r) = - < H I_r \us, H I_r \us> +< a_s,\L\us-\fs> +<a_r,\L^\top\ur-\fr>.
\eeq
By setting $\frac{\partial A }{\partial \us}=0$, I obtain
\beq
   \L^\top a_s = I_r^\top H^\top H I_r \us =  I_r^\top H^\top H R. 
\eeq
 The operator $H$ is diagonal and real, which means $H^\top=H$. This equation tells us to 
use the receiver wavefield $\ur$ as imaging operator together with the image residual $H^\top H R$.
 The wavefield $a_s$ travels from the image towards the source $\fs$. 

To find the adjoint receiver wavefield $a_r$, I redefine  equation \ref{eq:aug1}  to 
the equivalent form
\beq
    A(\m,\us,\ur,a_s,a_r) = - < H I_s \ur, H I_s \ur> +< a_s,\L\us-\fs> +<a_r,\L^\top\ur-\fr>.
\eeq
Now, we set $\frac{\partial A }{\partial \ur}=0$ to obtain the last adjoint variable $a_r$:
\beq
   \L a_r = I_s^\top H^\top H R.
\eeq

From the last optimality condition we obtain the actual gradient:
\beq
  \frac{\partial J_2 }{\partial \m}=  -\sum_{t,e}  \frac{\partial^2 \us}{\partial t^2}a_s + \frac{\partial^2 \ur}{\partial t^2}a_r.
\label{eq:gradJ2}
\eeq

\subsection{Main results}
 I demonstrate the proposed workflow with a real 2D marine dataset
acquired with a variable depth cable  \citep{soubaras2010variable}.
The streamer depth increases as a function of offset, which enhances the frequency
content of the data by producing a mixed notch response. Hence,
the increasing cable depths improve the low frequency content at 
intermediate and far offsets which is very helpful for data-domain waveform tomography (DWT). The acquisition setup 
in the modeling software mimics the
variable depth streamer cable and the free surface \citep{hicks2002arbitrary}, and hence it is consistent with the observed data .
  
 \rfg{image-winit} shows the initial image obtained from the model \rfg{vel-winit}, and  \rfg{anglex-image-winit}
shows the corresponding angle gathers. The initial model is constructed by time-domain moveout analysis followed
by Dix conversion to depth \citep{dix}. \rfg{image-wtomo} shows the updated RTM image
after tomography (described in the previous section). Observe the increased focusing shown in the updated image. 
 \rfg{anglex-image-wtomo} shows the corresponding gathers, whereas \rfg{vel-wtomo} is the updated velocity model. The
illumination-based tomographic step decreases the velocity along the section and produces an increase of flatness
in the gathers. \rfg{image-wfwi} is the RTM image corresponding to the waveform inversion model. Note the improved
focusing of the image in the shallow part of the section, which is more apparent in the angle gathers 
depicted in \rfg{anglex-image-wfwi}. The final model is shown in \rfg{vel-wfwi}. Note how further details are 
added into the velocity model, which changes the structure of the image and flattens in the
reflector at $z=2.2$km. 

The combination between image-domain and data-domain wavefield
tomography seeks to exploit the features of each method. The image-domain 
wavefield tomography methods are sensitive to the smooth components
of the model due to the definition of the inverse problem. Once I obtain
 a smooth model that improves focusing in the CIP gathers,
 I proceed to further refine the model using data-domain
 wavefield tomography. The image-domain wavefield tomography
model corrects for most kinematic errors, 
whereas
the data-domain wavefield tomography model 
corrects early arrival phase errors in the data, and
introduces  discontinuities in the model directly correlated with events in 
the image.





\inputdir{paperIII}

\multiplot{3}{image-winit,image-wtomo,image-wfwi}{width=\textwidth}%
{RTM images from (a) the initial model, (b) illumination-based tomographic model, and (c) data-domain inversion model.
Note how the focusing (image strength) increases from (a) to (b). Also, one can see the shallow part of the image
improved due to the data-domain tomography around $z=1.1$km.}


\multiplot{3}{anglex-image-winit,anglex-image-wtomo,anglex-image-wfwi}{width=\textwidth}%
{Angle gathers retrieved at sparse surface locations from (a) the initial model, (b) illumination-based tomographic model, and (c) data-domain inversion model. Observe how the flatness of the gather increases from (a) to (b). The shallow part of the gathers $z<1.2$km 
improves in (c).}


\multiplot{3}{vel-winit,vel-wtomo,vel-wfwi}{width=\textwidth}%
{Velocity profiles from (a) the initial model, (b) illumination-based tomographic model, and (c) data-domain inversion model.
Note how the velocity decreases from (a) to (b) and details are added in the model in (c).}







\newpage
\section{2- Seismic tomography using local correlation functions }
Here, I propose to use local
temporal correlation functions to extract the misfit information in the data
domain (at the receiver locations), instead of measuring such correlations
in a global sense, as suggested by other approaches \citep{van2010correlation,simon,warner}. To compute the local
correlation functions, I use the method of \cite{hale2006}, which performs
the local summation in an efficient manner using recursive Gaussian filters
\citep{hale2006recursive}. Local correlations minimize the cross-talk between
seismic events, since the comparison between observed and simulated waveforms
is performed locally in time. The cross-talk arises due to the correlation of 
events in two signals that do not correspond to each other. The correlation between
two distant events introduces complexity in the correlation that can be misleading.
 Instead, by preserving the correlation as a function of time, the chances 
of cross-talk a reduced and the correlation functions are simpler. 
This is a key advantage of this method because
it has the ability to produce cleaner adjoint sources, which translate
into cleaner gradients with less cross-talk than the global correlation
functionals.






Consider the problem of measuring the similarity between two signals $f(t)$ and
$g(t)$ (\rFgs{f}-\ref{fig:g}). One way is to measure the difference between
$f$ and $g$. This approach is highly sensitive if the two signals are close
enough (in a kinematic sense) to each other. However, if the two signals
contain a kinematic difference longer than half a period, this functional is
not informative \citep{Bunks95}. Instead of determining the similarity by
measuring the difference, one can determine the similarity through correlation. In the most
common case, one can use a global correlation function \citep{Tristan}. Here,
I follow the convention of \cite{hale2006} and define centered correlations:
%%%
\beq
  c(\tau) = (f\star g)(\tau) = \int \limits_{-\infty}^{\infty} f(t' -\tau/2)g(t'+\tau/2) dt'. 
  \label{eqn:gcorr}
\eeq
%%%
The correlation function $c(\tau)$ gives us a general sense of the similarity
between $f(t)$ and $g(t)$. Alternatively, one can estimate the similarity in a
localized way \citep{hale2006}, by capturing how the two signals are
related to each other along the time axis:
%%%
\beq
 c(t,\tau) = (f \star_l g)(t,\tau) = \int \limits_{-\infty}^{\infty} w(t'-t) f(t'-\tau/2) g(t'+\tau/2)dt', 
 \label{eqn:lcorr} 
\eeq 
%%%
where $w(t'-t) \equiv e^{-\tau^2/4\sigma^2}e^{-(t'-t)^2/\sigma^2}$ is a
Gaussian window centered at $t'$ and $\sigma$ its standard deviation.
 As $\sigma\rightarrow \infty$, the local correlations behave
like the global correlations at each time $t$, i.e. they are invariant with
respect to time $t$. This localized formulation is equivalent to convolving the
shifted product $h_\tau(t') = f(t'-\tau/2)g(t'+\tau/2)$ with a Gaussian window:
$ c(t,\tau) = (G * h_{\tau})(t)$. The convolution is efficiently implemented
using recursive Gaussian filters (for which the convolution cost is independent
of the Gaussian half-width $\sigma$) \citep{hale2006recursive}. One can think
about \req{lcorr} as a linear operator defined as ${\bf c} = {\bf C g}={\bf G
}{\bf S} {\bf g}$, where $\bf C= G S$ is the local correlation matrix, $\bf G $ is
a block diagonal matrix (with each block being a convolution matrix with the
Gaussian function), and $\bf S$ contains shifted versions of $f$. The 
adjoint operator ${\bf C}^\top$ implements a local convolution as ${\bf
C}^\top = {\bf S}^\top {\bf G}$. By construction, the matrix $\bf G$ is symmetric positive definite (SPD)
since all of the blocks are SPD: hence, ${\bf G}^\top = \bf G$. Mathematically,
 the adjoint operator is implemented as follows:
\beq
  f^* (t)  = \int \limits_{\tau_{min}}^{\tau_{max}} \lp \int\limits_{-\infty}^{\infty} w(t'-t+\tau/2) c(t'+\tau/2,\tau) dt' \rp g(t+\tau) d\tau,  
  \label{eq:adjt}
\eeq
where $\tau_{min}$ and $\tau_{max}$ are the minimum and maximum time-lags, respectively, and $f^*$ represents 
the retrieved function from the application of the adjoint operator.


\inputdir{paperIV}
\multiplot{4}{f,g,gcorr,lcorr}{width=0.45\textwidth} {Signals
(a) $f(t)$ and (b) $g(t)$ to exemplify the correlation functions, (c) is the
global correlation $f\star g$ (invariant with time), and (d) $f\star_l g$ is
the local correlation with a Gaussian window with $\sigma=0.3s$.}





\rfg{gcorr} shows the global correlation between the signals in \rFgs{f} and
\ref{fig:g}. Note that the global correlation is time-invariant and contains
crosstalk between events (around $\tau=-0.3$s and $\tau=0.3$s); the crosstalk is
captured because the correlation lags are big enough to facilitate interference
between different events. In contrast, \rFg{lcorr} shows the local correlation
between $f(t)$ and $g(t)$; no crosstalk is observed because the Gaussian
window prevents interference between distant events. However, it 
is still possible to observe crosstalk between events using local correlations if the events
lie within the correlation window of standard deviation $\sigma$.

\subsection{Tomography setup}

In this chapter, I  show the application of correlation functionals for the 
inverse tomographic problem under the physics of the acoustic wave equation (equation~\ref{eq:we}).
 For the remainder of this chapter, I 
express the wave equation as the linear operator $\Lop(\m)$ with its corresponding
adjoint (time reversal) operator $\Lop^\top(\m)$.
 I design a correlation-based objective
function that maximizes the correlation between observed and modeled data:
%%%
\beq
  J(\m) = \frac{\ltnorm{P(\tau) d^{o}\star_l d^{\m}}}
           {\ltnorm{d^{o}\star_l d^{\m}}}=
          \frac{\ltnorm{ {\bf P C d^\m}}}{\ltnorm{{\bf C d^\m}}},
  \label{eqn:of}
\eeq
%%%
where $P(\tau)$ is a penalty operator that enhances energy outside $\tau=0$
(which I want to minimize), $\m$ is the model parameter, and $d^o(e,\xx_r,t)$
and $d^m(e,\xx_r,t)$ are the observed and modeled data for each experiment $e$,
respectively. The correlation matrix $\bf C$ is built using shifted versions
of the observed data $d^o$ along with the Gaussian convolution, as defined
by the process in equation~\ref{eqn:lcorr}. The application of the penalty
operator $P(\tau)$ is equivalent to a diagonal penalty matrix $\bf P$. Here,
the L2 norm squared $\ltnorm{.}$ is defined over receiver coordinates $\xx_r$,
experiment index $e$, time $t$, and correlation lag $\tau$.

 The objective function in equation~\ref{eqn:of} is similar to that of
\cite{Tristan} except that I use local instead of global
correlation, and I normalize the denominator. The normalization
balances the objective function by the energy of the correlation function
\citep{warner}. This step eliminates the trivial solution of 
$d^{\m} = \vec{0}$. 


To solve the optimization problem, I use gradient descent methods. 
Similar to the previous chapter, I use the adjoint
state method framework \citep{plessix} to obtain the gradient by
defining the problem under equality constraints given by the wave equation as
%%%
\beq
  \begin{aligned}
    & \underset{\m}{\text{minimize}}
    & & J(\m) \\
    & \text{subject to}
    & & \L u = \fs.
  \end{aligned}
\eeq
%%%
 These constraints satisfy the wave equation for the modeled data $d^\m$ for
every experiment $e$. The augmented functional $A$ is defined as
\beq
  A(u_s,a_s,\m) = J(\m)  - \sum_e< a_s, \L u_s -\fs >,
  \label{eqn:lagrangian}
\eeq
from which one obtains the appropriate Lagrange multipliers $a_s(e,\xx,t)$        
(adjoint wavefields) that satisfy the constrained equation.

The stationary points of the Lagrangian provides the equations to solve 
the problem. The first condition
\beq
\frac{\partial A }{\partial a_s} =0
\eeq  
leads to the state variable problem 
\beq
  \Lop (\m) u_s(e,\xx,t) = \fs(e,\xx,t)
\eeq
for each experiment $e$. The second condition
\beq
 \frac{\partial A }{\partial u_s} = 0
\eeq
leads to the adjoint state variable system
\beq
  \Lop^\top(\m) a_s(e,\xx,t) = g_s(e,\xx,t),
\eeq
where $g_s(e,\xx,t)$ is the adjoint source for this specific choice of objective
function.
 The adjoint sources are given by
\beq
  g_s = -\frac{1}{\ltnorm{{\bf C }d^\m}}{\bf C^\top}\lp {{\bf P^\top P} -J{\bf I}}\rp {\bf C } d^\m. 
\eeq
%%
 This expression indicates that one has to twice penalize the correlation and subtract
the value of the objective function from it before computing the adjoint
correlation. This process effectively balances the influence between the
numerator and denominator in the adjoint source. At early iterations, when $J$
is larger, the norm of the correlation has greater influence on the adjoint
source; as iterations continue, the penalty term becomes more prominent.
 Once one solves for $u_s$ and
$a_s$, the gradient of $J$ is computed with the last condition:
%%
\beq
  \frac{\partial A }{\partial \m} = 0,
\eeq
from which I obtain the gradient
\beq
    \frac{\partial J }{\partial \m}= \sum_{t,e} \frac{\partial^2 \us}{\partial t^2}a_s,
\label{eq:corrGrad}
\eeq
where the double dot indicates the second derivative in time. Compared with 
the image-domain gradient in equation~\ref{eq:gradJ2} one can see that here
we only have a sensitivity path that connects sources and receivers directly. Instead,
 in image-domain, the gradient is constructed from the source $\xx_s$ to the image point $\xx$
and from the image point $\xx$ to the receivers $\xx_r$. That is why for the method in equation~\ref{eq:corrGrad}
one correlates two wavefields, whereas in equation~\ref{eq:gradJ2} we use four wavefields
to construct the gradient. 

\subsection{Main results}

\rFgs{gr-penaltyBP-global-vel-neg},~\ref{fig:gr-penaltyBP-global-vel-zer},
and \ref{fig:gr-penaltyBP-global-vel-pos} show the gradients from the global
correlations. Strong crosstalk between events is visible as high wavenumber
(reflection-like) events coincident at the free surface and elsewhere. However, the gradients
from local correlations (\rFgs{gr-penaltyBP-local-vel-neg},~\ref{fig:gr-penaltyBP-local-vel-zer},
and \ref{fig:gr-penaltyBP-local-vel-pos}) present the correct gradient direction
without the contamination from crosstalk, which is non-existent for the
chosen window.
 In this simple setting, I expect two sensitivity paths:
(1) from source to receiver, and (2) from source to the free surface and
from the free surface to the receivers. However, one can observe several more
events in the global correlation gradients. These extra events come from the
crosstalk present in global correlations, which is then propagated into the
adjoint wavefields. The arrows in the sensitivity kernels highlight the crosstalk
seen in the global correlation gradients.  

\multiplot{6}{gr-penaltyBP-global-vel-neg,gr-penaltyBP-local-vel-neg,gr-penaltyBP-global-vel-zer,gr-penaltyBP-local-vel-zer,gr-penaltyBP-global-vel-pos,gr-penaltyBP-local-vel-pos}{width=0.48\textwidth} {Sensitivity kernels for an
experiment with free surface boundary condition and a Gaussian anomaly placed
in between the source and the receiver (for which the surface reflection
event is not sensitive). The left and right columns show gradients for global
 and local correlations, respectively. From top to bottom, the
gradients correspond to low, correct, and high velocity. The arrows in the
left column highlight the cross-talk present in the global correlation gradients.}


Local as opposed to global correlations provide cleaner measures of similarity
between two functions. The main difference is that local correlation performs
a local sum instead of summing over all the time samples. This is the key to
eliminating the comparison between unrelated seismic events present in the
observed and modeled data. This feature allows to extract local kinematic
errors that are more instructive than the misfit extracted using global
correlations. Less cross-talk in the correlations translates into cleaner
adjoint sources, which can produce more informative model updates than those of
global correlation framework.
 The adjoint operator for the local correlations method suggests several
possible applications including objective functions based on local deconvolution,
 which extends further to local-matched filters applications. 








\newpage
\section{3- Extended reflectivity response using two-way wavefields}




In this section, I describe the Marchenko modeling framework which is able to correctly
handle primaries as well as  internal, and surface-related multiples. The modeling operator
can be thought as an alternative to the finite difference extrapolation used in RTM to solve for 
equation~\ref{eq:we}.
 \cite{Singh2015} extend the method of \cite{Wapenaar} and \cite{Fil2012} to handle surface-related
multiples. Their method
describes the total Green's function $G(e,\xx,t)$ which includes
primaries, internal, and surface-related multiples. One can think about the total
field $G$ as the sum of the source and receiver wavefields $\US$ and $\UR$ from previous chapters. 
 The main difference is how the wavefields $\US$ and $\UR$ are obtained. Here the 
solution to equation~\ref{eq:we} is obtain through the Marchenko algorithm, whereas 
in the previous chapters the solutions are obtain with finite difference modeling.  


For every point $\xx$ in the model, two focusing $f_1$ and $f_2$ solutions are found. 
 The solutions focus at the image point $\xx$ when combined as $f = f_1+f_2$, in other words 
when injected at the receiver locations the solutions produce a focus at the virtual source
location $\xx$. 
Once the focusing solutions are found through the algorithm described by \cite{wapenaar2014green} (and 
extended by \cite{Singh2015} to handle free surface multiples), the upgoing and downgoing 
components of the Green's function G are found. The directional components are complementary and satisfy 
the relation
\beq
 G(\xx,e,t) = \US(e,\xx,t) +\UR(e,\xx,t).
\eeq
 In order to solve for the focusing solutions and for the components of the total wavefield $G$, the algorithm
is initialized with an estimate of the transmission response that describes the first arrival from the point 
$\xx$ to the acquisition surface. The input data are deconvolved before the iterative scheme starts. This 
preprocessing step is done to avoid convolving the source function (encoded in the data) at each iteration. The retrieved
Green's function $G$ is bandlimited due to the wavelet contained in the first arrival (used to initialize the
iterations).  

In the Marchenko context, the wavefield $\UR$ represents the upgoing component of the Green's function, 
whereas $\US$ represents the downgoing component. 
 Following the imaging principle \citep{unified} the image $R(\xx)$ is constructed 
when wavefields $\US$ and $\UR$ coincide in space and time:
\beq
  R(\xx) = \sum_{e,\omega}  \US(e,\xx,t){\UR}(e,\xx,t),
\eeq
 Alternatively, the 
image can be found through the process of Multi Dimensional Deconvolution (MDD) proposed in \cite{Joost},
which solves the following linear problem:
\beq
  \UR(e,\xx',t) = \int_{\partial D_i} d\xx \int_{-\infty}^{\infty}  \US(e,\xx,t-\tau) R(\xx',\xx,\tau) d\tau,
  \label{eq:multi} 
\eeq
where $\xx$ and $\xx'$ are two arbitrarily positions in the depth level $\partial D_i$.  

This equation can be rewritten  with the following linear system
\beq
  {\bf u_r} = {\bf U_s} {\bf r},
\eeq
where $\bf r$ is the column vector representation of $R$, ${\bf u_r}$ is the column vector
representation of $\UR$, and $\bf U_s$ is a matrix that contains shifted versions of 
the source wavefield $\US$ and its action is equivalent to the integral in equation~\ref{eq:multi}.


We are interested in solving for the reflectivity $\bf r$, which can be achieved 
through the least squares solution:
\beq
  {\bf U_s}^\top{\bf U_s} {\bf r}={\bf U_s}^\top {\bf u_r}. 
  \label{eq:mddinv}
\eeq
Once the reflection response is found, the seismic image is defined as  
\beq
R(\xx) = R(\xx,\xx'=\xx,\tau=0). 
\eeq
Equation 34 finds a reflectivity response that obeys the scattering relation between 
the source wavefield and the receiver wavefield at the reflector position. It simply says that
the scattered wavefield $\UR$ must be equal to the incident wavefield $\US$ scaled by the 
reflection coefficient $R$. Since we deal with bandlimited wavefields, the integration over
positions $\xx$ and  time lag $\tau$ implies that the influence of the reflectivity in the
scattered wavefield does not arise from a unique point; instead it is generated over an 
area and around the time where both wavefields interact. The integral is solved for
all possible combinations between $\xx'$ and $\xx$ in the depth level $\partial D_i$; also,
the time lag axis is of the same length of the time axis $t$. These limits of integration 
can be very costly; instead, the area and time where the wavefields interact should be related
to the dominant period and wavelengths present in the wavefields $\US$ and $\UR$. 
The estimated extended reflectivity $R$ must satisfy the scattering relation as best as possible for 
every experiment $e$ (i.e., in a least squares sense). Equation 34 does 
not rely on the Marchenko wavefields; in fact, it can be applied to wavefields extrapolated
with conventional propagators.  

\subsection{Connecting Marchenko and conventional imaging}

In terms of imaging, the reflectivity response in equation~\ref{eq:multi} and the extended 
imaging condition in equation~\ref{eq:eic} show how the two wavefields $\US$ and $\UR$ interact
in the subsurface as a function of space $\hh$ and time $\tau$.
 Redefining $\xx' = \xx + \hh$ and  $t'=\tau$ and substituting it in equation~\ref{eq:multi} yields
a relation similar to the adjoint of the extended imaging condition (equation \ref{eq:adj1}).


More importantly, the reflection response $R(\xx,\hh,\tau)$ contains important information
about the background model, and can be used for tomography. The main difference between
the MDD reflection response and the extended images is that the earlier 
is solved through the inverse problem described in equation \ref{eq:mddinv}, whereas
the later is approximated by
\beq
  {\bf r } = {\bf U_s}^\top {\bf u_r}.
\eeq
Hence, it assumes ${\bf U_s}^\top{\bf U_s} \approx {\bf I}$.


\subsection{Preliminary results}

In this chapter, I describe an alternative to computing $\US$ and $\UR$ through 
the so-called Marchenko framework. Additionally, the reflection response can be computed 
through the MDD process, or directly by extended imaging. 
 Here, I show a comparison between conventional and Marchenko extrapolated wavefields. 
Figures \ref{fig:ro} and \ref{fig:shot250}  the density model  and the 
retrieved data in an experiment with a free surface with a constant background velocity. The reflections observed 
in the data are due to discontinuities in the density model and also due to the free surface. 
 I compute extended images with wavefields extrapolated with the RTM and Marchenko
engines. Also, I introduce an error in the background velocity field 
to analyze the sensitivity of the Marchenko framework to errors
in the background model. 
 Figures \ref{fig:eic_Gm_perc_090}, \ref{fig:eic_Gm_perc_100}, and \ref{fig:eic_Gm_perc_110}
show the extended images obtained with the Marchenko wavefields for 
velocity errors of -10\%, 0\%, and +10\%, respectively. 
  Figures \ref{fig:eic_wfl_perc_090}, \ref{fig:eic_wfl_perc_100}, and \ref{fig:eic_wfl_perc_110}
show the corresponding extended images with the conventional wavefields. 
 Both methods show sensitivity (given by the defocusing) to the background velocity 
model. An obvious difference is that the images obtained with the Marchenko 
wavefields correctly image the free surface multiples in the right place, whereas
the images obtained with the conventional wavefields show the image of
the surface-related multiples as reflectors in the model. However, the Marchenko 
images show a fake reflector at $z=0.1$km. In this model there are three reflectors:
 the free surface and the interfaces shown in the density model. The fake event
in the Marchenko gather seems to be an expression of the second interface mapped below the free surface.   




%\inputdir{marchenko}

%\multiplot{3}{00_low,01_correct,02_high}{width=0.3\textwidth}%
%{Preliminary results showing the sensitivity of the imaging process using the Marchenko modeling 
%framework to errors in the background model: (a) a -20\% velocity error, (b) the correct 
%background velocity, and (c) a +20\% velocity error. Note how the multiples below the deepest
%reflector are very weak with respect to the amplitude of the imaged interfaces. }

\inputdir{forMarchenko}
\multiplot{3}{ro,shot250}{width=0.7\textwidth}{Free surface experiment with a constant velocity background: (a) variable density model and (b) the recorded data for a shot at $\xx_s=(0,0)$km. Note how the data contain primaries (first two arrivals), internal multiples (weak third arrival), and the corresponding surface-related multiples.  }

\multiplot{6}{eic_Gm_perc_090,eic_Gm_perc_100,eic_Gm_perc_110,eic_wfl_perc_090,eic_wfl_perc_100,eic_wfl_perc_110}{width=0.3\textwidth}{Preliminary results showing the sensitivity of the extended imaging process using the Marchenko modeling (top row) and RTM modeling (bottom row) framework to errors in the background model: (a) and (d) a -10\% velocity error, (b) and (e) the correct 
background velocity, and (c) and (f) a +10\% velocity error. Note how in the top row space-lag gathers 
the surface-related  multiples below the deepest
reflector are very weak with respect to the amplitude of the imaged interfaces. In contrast, with the RTM modeling operator, the
 surface-related multiples are imaged as primaries. }




\newpage
\section{4- Wavefield tomography with multiple scattered waves}
 Multiple scattered wavefields travel through the earth
along many different paths compared to the more limited paths experienced by 
single scattered waves. If one could use the full scattering 
nature of the recorded data, the sensitivity to errors in the
background model can greatly increase. In conventional tomography we
consider algorithms (i.e., chapter I and chapter II) that follow
the Born model (single scattering). However, the 
recorded wavefield contains surface-related and internal multiples. These
waves travel along many different paths in the background model. Hence, 
if we are able to measure kinematic errors from the full wavefield 
with primaries and multiples we could better update the background model.  


In order to illustrate the increase in sensitivity, let us
consider a simple synthetic example. 
\rfgs{vel}-\ref{fig:den} show a velocity and a density model, respectively.
 The reflectivity in the model comes from the density contrast. The source 
and receiver coordinates are at the surface $6$km apart from each 
other, i.e.  at $\xx_s=(1.0,0.0)$km and $\xx_r=(7.0,0.0)$km, respectively. 
I recreate a Born modeling experiment (single scattering) 
by using an absorbing boundary condition at the surface during wavefield simulation. Likewise,
 to understand the multiple scattering case, I use a free surface boundary condition which
leads to significant surface-related multiples. 
  To recreate the sensitivity kernels, I first model data from the source location $\xx_s$ 
and record them at the receiver location $\xx_r$. Then, I backward propagate the data
into the model and obtain the sensitivity kernels by correlating the forward propagated
wavefield with the backpropagated recorded wavefield. I only keep the correlation 
of events that travel along the same direction in space  and time by applying a directional
imaging condition \citep{liu:S29}. 
   \rfg{ss-xx-kernel-fsm} shows the kernel obtained in the single scattering 
mode (absorbing boundary condition). It shows that a velocity perturbation is likely 
to happen in an area connecting the source with the receiver at various locations along
the reflector.
The spread of the kernel is related to the bandwidth of the data; the larger the bandwidth
the narrower the kernel paths become. The kernel says that any property outside the correlation
path should not change the recorded wavefield at receiver $\xx_r$. 

\rfg{ss-xx-kernel-f} shows the kernel for the data recorded with a free surface boundary
condition. The wavefield correlation contains different combinations of wavefields that bounce
at  the surface and on the reflector and are recorded are  $\xx_r$. 
 It shows that the sensitivity of the multiple-scattered data to the background model is much 
greater than that of the  single-scattered data.  





\inputdir{kernels}
\multiplot{4}{vel,den,ss-xx-kernel-fsm,ss-xx-kernel-f}{width=0.9\textwidth}%
{Band limited sensitivity kernels: (a) shows the background velocity model, (b) is the density model, (c) is an image
kernel under the single scattering assumption connecting source $\xx_s =(1.0,0)$km  and
  image point $\xx=(4.0,1.0)$km with receiver point at $\xx_r=(7.0,0.0)$km, and (d) the sensitivity 
kernel connecting $\xx_s-\xx-\xx_r$ with every possible path given by the multiples and primaries. }


\subsection{Retrieving the rough components of the model}
  The reconstructed Marchenko wavefields depend on the kinematics of the background model used to simulate 
the direct arrivals from each point $\xx$ in the subsurface. Interestingly, the 
retrieved wavefield show reflections even if the background model
is smooth. The reflections are not generated by the background model, but by the recorded data during the solution of the
Marchenko equation. That means that the wavefields follow the background model and at the same time follow
the reflectivity of the original model. This can be explained by the relation 
\beq
    u = \US+\UR  = u_0 +\delta u.
\eeq
where $u$ is the total Marchenko wavefield, $u_0$ is the background wavefield that travels from 
the source $\xx_s$ to the model $\xx$ in the background model (the first arrival), and 
 $\delta u$ the scattered wavefield (it travels in all directions). Hence, if the background model $\m_0$ explains the 
kinematics of the true model, then the retrieved wavefield follows the homogeneous wave equation 
for the true model:
\beq
    \m(\xx) \omega^2 u(\xx,e,\omega) +\nabla^2 u(\xx,e,\omega) =0.
\eeq 

The model $\m$ can be solved directly in a least squares sense for all experiments $e$. Alternatively,
 one could solve for the perturbation $\delta \m$ that explains the wavefield $u$:
\beq
    (\m_0(\xx) + \delta \m (\xx)) \omega^2 u(\xx,e,\omega) +\nabla^2 u(\xx,e,\omega) =0,
\eeq
from which we can solve for the model perturbation $\delta m(\xx)$:
\beq
    -\delta \m (\xx) \omega^2 u(\xx,e,\omega)  = \m_0 (\xx) \omega^2 u(\xx,e,\omega)  + \nabla^2 u(\xx,e,\omega)
  \label{eq:nlb}
\eeq
Equation ~\ref{eq:nlb} resembles the Born modeling formula. In Born modeling, the incident wavefield (left hand side) is 
approximated by the background wavefield  $u_0$:
\beq
    -\delta \m (\xx) \omega^2 u_0(\xx,e,\omega)  = \m_0 (\xx) \omega^2 u(\xx,e,\omega)  + \nabla^2 u(\xx,e,\omega).
  \label{eq:lb}
\eeq
In equation 42 no approximation is needed since the total wavefield $u$ is solved independently through the Marchenko modeling framework. 
Solving equation ~\ref{eq:nlb} for the perturbation $\delta\m$ can be regarded as a nonlinear inversion were 
all scattered modes are used for the inversion, as opposed to least squares migration were the perturbation is only due to 
the single scattered field.
 The important message in this chapter is that multiples participate directly in the tomography process, without requiring 
any special treatment. The wavefields could be used to update the background model or a model perturbation 
as is done in least squares migration. 


\documentclass[10pt]{article}
\usepackage{amsmath, amsthm, amsbsy, rotating,float}
\usepackage{graphicx,algorithm,algorithmic}
\usepackage{setspace,enumerate}
\usepackage[margin=1.0in]{geometry}
\usepackage{graphicx,caption,subfig}

\floatstyle{ruled}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert_p}
\newcommand{\normi}[1]{\left\lVert#1\right\rVert_\infty}
\newcommand{\normt}[1]{\left\lVert#1\right\rVert_2}

\doublespacing
\author{Esteban D\'{i}az}
\title{Homework 3}{}
\floatstyle{ruled}
\newfloat{program}{thp}{lop}
\floatname{program}{Program}
\newfloat{printout}{thp}{lop}
\floatname{printout}{Printout}
\DeclareMathOperator{\sinc}{sinc}

\begin{document}

\maketitle

\section{Problem 1}
Let $A\in R^{m \times m}$ be nonsingular. We want to prove that $A$ have an unique
$LU$ factorization. 

\begin{proof}
  Suppose $A=L_1U_1 = L_2U_2$, with $L_1,L_2$ being unit lower
triangular, and $U_1,U_2$ being upper triangular matrices. Given that
A is not singular, then the entries in 
the diagonal of $L_1,L_2,U_1,U_2 \neq 0$. This makes the upper and lower 
triangular matrices from the decomposition to be non-singular, since
$det L_i \neq 0$ and $det U_i \neq 0$.

Since the matrices are nonsingular we can write
\[
  U_1U_2^{-1} = L_1^{-1}L_2
\]
In problem 1 of hw 1 we proved that the inverse of a nonsingular
 lower or upper triangular is also lower or upper triangular, respectively.

Following the same logic as in problem 1, hw 1 we can also proof that
the product of two upper or lower triangular matrices is also 
upper or lower triangular, respectively.

Combining these two facts, we can write:
\[
  U_1U_2^{-1} = U= L= L_1^{-1}L_2
\]

Now, the only way the equality holds is if $U,L$ are diagonal matrices,
but since $L$ is \textbf{unit} lower triangular, then:
\[
  U = L = I
\]
which implies $U_1U_2^{-1}=I \Rightarrow U_2=U_1$, the same happens
for $L_1 = L_2$. Hence the factorization is unique.
\end{proof}


\section{Problem 2}
Let $A\in R^{m \times m}$. Let's consider step $k$, with $1\leq k\leq m$.

To proof that at least one entry $a_{i,k}^{k}$ of matrix $A^{(k)}$
is different from zero, we can inspect the case where all entries 
$a_{i,k}^{(k)}=0, i=k,\dots,m$. If that is the case, then we cannot
create a $1$ on the main diagonal. Furthermore, this would imply that
the matrix $L$ as a zero on the main diagonal which result in a 
contradiction because $L$ is lower unit triangular.
This implies that $A$ is nonsingular, which is against our initial assumption. Let's say $m=3$:
  \[
    A^{(1)} = 
    \begin{bmatrix}
        x  & x & x \\
        x  & x & x \\
        x  & x & x \\
     \end{bmatrix}
  \]
  after step (1) we have:
  \[
    A^{(2)} = 
    \begin{bmatrix}
           x & x \\
           x & x \\
     \end{bmatrix}
  \]
  if 
  \[
    A^{(2)} = 
    \begin{bmatrix}
           0 & x \\
           0 & x \\
     \end{bmatrix}
  \]
  it means that $A$ is singular which contradicts our initial assumptions.


\section{Problem 3}
From the results in a, we can say that both methods are comparable
since they yield similar (same order) of relative error. In class
we saw that the error of of the methods in the exact arithmetics 
is:
  \[
  \frac{\normi{\tilde{L}\tilde{U}-A}}{\normi{A}} \leq cu
  \]
with $u\approx 10^{-16}$ being the computer round off error, and
c is a constant not too large.

For part (a) the error from both methods are relatively small. However,
 GEPP are always smaller. In part (a) the constant $c$ ranges from 15 to 
126 for GE, and 1.12 to 6 for GEPP.

For part (b) the error of GE algorithm grow considerably. However, for 
GEPP the relative errors remained in the same order. This confirms 
that GEPP is backward stable, and the $\tilde{L}\tilde{U}$ is the
exact factorization for slightly perturbed data.

\section{Problem 4}
The Pascal matrix has integer entries that are machine numbers for 
$n=12$. The vector $\vec{b}$ is generated by multiplying 
$A\vec{x}$, where $x = [1,1,1,1,...,1]^T$. The entry
$b_i$ is equal to the product and sum of machine numbers. Hence,
the entries of $\vec{b}$ are also machine numbers. We can say that:
$A \in F^{m \times m}$ and $\vec{b} \in F^m$.
 

Hence, the error we see in the method gives us an idea of the
performance of the method. I got a relative error of order $10^-7$. So,
 we can expect at least 7 significant digits.



\section{Problem 5}
Assume $A\in R^{m \times m}$ is SPD.
\subsection{i}
  Let $X\in  R^{m \times m}$ is nonsingular, then we want to verify
that 
  \[
  \vec{y}^T X^TAX\vec{y} >0; \vec{y}\neq \vec{0}\in R^m 
  \]
  we can define $z=X\vec{y} \neq \vec{0} \in R^m$, since $X$ is 
  nonsingular; then it follows that
  \[
  \vec{z}^TA \vec{z} >0
  \]
  is also SPD.
\subsection{ii}
  To show that each submatrix is SPD, we can consider the following.
  \begin{proof}
    By definition a matrix is SPD is $vec{x}^TA\vec{x} >0$, where
  $\vec{x}\neq \vec{0} \in R^m$.
    
    Testing that each principal submatrix $A'=A(j:k,j:k)$ is SPD, is 
  equivalent to use the following test vector $\vec{x}$.

  \[
    \vec{x} = 
    \begin{bmatrix}
        0  \\
        \vdots \\
        x_j \\
        \vdots \\
        x_k \\
        \vdots \\
        0
     \end{bmatrix}_{m\times 1}
  \] 
  Hence the SPD test is:
 \[
    \begin{bmatrix}
        0  &
        \dots &
        x_j &
        \dots &
        x_k &
        \dots &
        0
     \end{bmatrix}_{1\times m}
    \begin{bmatrix}
        a_{11}  & \dots & \dots & a_{1m} \\
         \vdots & \ddots & \vdots & \vdots \\
         \vdots & \dots & \ddots & \vdots \\
        a_{1m}  & \dots & \dots & a_{mm} \\
     \end{bmatrix}_{m\times m}
      \begin{bmatrix}
        0  \\
        \vdots \\
        x_j \\
        \vdots \\
        x_k \\
        \vdots \\
        0
     \end{bmatrix}_{m\times 1} >0
 \]  
  also satisfies the SPD test. Hence, each submatrix is also SPD. 
  Which is equivalent to:
 \[
    \begin{bmatrix}
        x_j &
        \dots &
        x_k &
     \end{bmatrix}_{1\times k-j+1}
    \begin{bmatrix}
        a_{jj}  & \dots & \dots & a_{jk} \\
         \vdots & \ddots & \vdots & \vdots \\
         \vdots & \dots & \ddots & \vdots \\
        a_{jk}  & \dots & \dots & a_{kk} \\
     \end{bmatrix}_{ k-j+1\times  k-j+1}
      \begin{bmatrix}
        x_j \\
        \vdots \\
        x_k \\
     \end{bmatrix}_{ k-j+1\times 1} >0
 \] 

  \end{proof}





\begin{program}
\begin{verbatim}
--------------------
     N=10      

mynorm =

   3.9642e-16


mnorm =

   1.1388e-16

--------------------
     N=20      

mynorm =

   1.0877e-15


mnorm =

   2.0762e-16

--------------------
     N=40      

mynorm =

   4.8175e-15


mnorm =

   2.3083e-16

--------------------
     N=80      

mynorm =

   9.9851e-14


mnorm =

   6.4857e-16
\end{verbatim}
  \caption{Problem 3: printouts (part a)}
\end{program}


\begin{program}
\begin{verbatim}
--------SMAL a11 --------
--------------------
     N=10      

mynorm =

   2.8089e+00


mnorm =

   1.3734e-16

--------------------
     N=20      

mynorm =

   5.3764e-01


mnorm =

   1.9297e-16

--------------------
     N=40      

mynorm =

   4.9040e-01


mnorm =

   2.7889e-16

--------------------
     N=80      

mynorm =

   5.8816e-01


mnorm =

   6.7047e-16
\end{verbatim}
  \caption{Problem 3: printouts (part b)}
\end{program}


\begin{program}
\begin{verbatim}
clear; clc;
format short e
ns = [10,20,40,80];


u = eps/2;
for i=1:4
  N= ns(i);
  A = randn(N);
  [L1 U1] = gauss(A);
  [L2 U2 P] = lu(A);

  fprintf('--------------------\n')
  fprintf('     N=%d      \n',N) 
  mynorm = norm(L1*U1-A,inf)/norm(A,inf)
  mnorm = norm(L2*U2-P*A,inf)/norm(A,inf)
end


fprintf('--------SMAL a11 --------\n')
for i=1:4
  N= ns(i);
  A = randn(N);
  A(1,1) = u;
  [L1 U1] = gauss(A);
  [L2 U2 P] = lu(A);

  fprintf('--------------------\n')
  fprintf('     N=%d      \n',N) 
  mynorm = norm(L1*U1-A,inf)/norm(A,inf)
  mnorm = norm(L2*U2-P*A,inf)/norm(A,inf)
end

\end{verbatim}
  \caption{Problem 3: driver}
\end{program}
\begin{program}
\begin{verbatim}
function [L,U] = gauss(A)

[n,m] = size(A);
U=A;
L=eye(m);

for k=1:m-1
  for i=k+1:m
    multiplier = U(i,k)/U(k,k);
    L(i,k) = multiplier;
    for j=k:m
      U(i,j)=U(i,j)-multiplier*U(k,j);
    end
  end
end
\end{verbatim}
  \caption{Problem 3: gauss}
\end{program}


\begin{program}
 \begin{verbatim}
  clear; clc;
format  long e

A = pascal(12);
x = ones(12,1);
[L U P]=lu(A);

b = A*x;
c=P*b;
y=forwsub(L,c);
x_1 = backsub(U,y);

relative_error = norm(x-x_1,inf)/norm(x,inf);

k = cond(A,inf);
max_err = k*eps/2;


Printout:

x_1 =

     9.999999997916706e-01
     1.000000006434592e+00
     9.999999477372815e-01
     1.000000208897596e+00
     9.999994978381744e-01
     1.000000794421329e+00
     9.999991375191816e-01
     1.000000650791341e+00
     9.999996628616827e-01
     1.000000114800716e+00
     9.999999767904109e-01
     1.000000002116025e+00


relative_error =

     8.624808184309884e-07


max_err =

     1.930689212757272e-04


  \end{verbatim}
\caption{problem 4: driver}
\end{program}

\end{document}
